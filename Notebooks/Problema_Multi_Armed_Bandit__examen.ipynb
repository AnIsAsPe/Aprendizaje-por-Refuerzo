{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problema Multi Armed Bandit_ examen.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNntmoiBIngOriDDpCWgL5e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnIsAsPe/Aprendizaje-por-Refuerzo/blob/main/Notebooks/Problema_Multi_Armed_Bandit__examen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9ADAbH73rfS"
      },
      "source": [
        "# Examen Práctico de Aprendizaje por Refuerzo\n",
        "\n",
        "El siguiente código utiliza una estrategia puramente de exploración para encontrar el valor esperado del premio que otorga cada máquina o brazo en el probema \"Multi Armed Bandit Problem\".\n",
        "\n",
        "## Instrucciones\n",
        "\n",
        "1. Diseñar una estrategia codiciosa (greedy strategy), también llamada estrategia de explotación, para encontrar el valor esperado de cada máquina,\n",
        "2. Diseñar una estrategia que priorice la exploración o la explotación según el progreso del experimento (epsilon decreasing greedy), para encontrar el valor esperado de cada máquina\n",
        "3. Explicar y comparar las tres estrategias ((exploración pura, codiciosa y epsilon codicioso decreciente) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ0Ts9FIHEmu"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTvgN0tvHi3A"
      },
      "source": [
        "def entorno_multi_armed_bandit(maquinas):\n",
        "    '''\n",
        "    Creamos el entorno para el problema \"multi_armed_bandit\" generando aleatoriamente\n",
        "    la distribución de probabilidad de los premios que otorga cada máquina\n",
        "    '''\n",
        "    medias = np.random.normal(size=maquinas)*5\n",
        "    std_ = np.random.uniform(0, 5, size=maquinas)\n",
        "    return medias, std_"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srAAbKjhKM_E"
      },
      "source": [
        "def init_Q(maquinas):\n",
        "  '''Inicializa el vector Q en ceros'''\n",
        "  Q =np.zeros((1, maquinas))\n",
        "  return Q"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2yLnkZKLUlp"
      },
      "source": [
        "def selecciona_maquina(maquinas):\n",
        "    '''selecciona una máquina aleatoriamente con distribución unifome'''\n",
        "    selec = np.random.choice(range(maquinas))\n",
        "    return selec"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUv7oB7VId1t"
      },
      "source": [
        "def calcula_recompensa(selec):\n",
        "  '''calcula la recompensa de jugar en una determinada máquina'''\n",
        "  r = int(np.random.normal(medias[selec], std_[selec], 1))\n",
        "  return r"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHfwiInqOkBX"
      },
      "source": [
        "def actualiza_Q (Q, selec, r, episodio):\n",
        "    '''actualiza el vector con los valores esperados de recompensa'''\n",
        "    Q[0, selec] = Q[0, selec] + 1/(episodio)*(r - Q[0, selec])\n",
        "    return Q"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGdvub9qSwmJ"
      },
      "source": [
        "### Definimos el número de máquinas o brazos del problema\n",
        "n = 4\n",
        "\n",
        "### Creamos el entorno\n",
        "medias, std_ = entorno_multi_armed_bandit(n)  # inicializa la distribución de probabilidad de cada máquina\n",
        "\n",
        "### Inicializamos el vector Q  \n",
        "Q = init_Q(n)  \n",
        "\n",
        "episodios = 100000 \n",
        "for episodio in range(1,episodios+1):\n",
        "  selec = selecciona_maquina(n)\n",
        "  r = calcula_recompensa(selec)\n",
        "  Q = actualiza_Q(Q, selec, r, episodio)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxGyo-3E-nyr",
        "outputId": "bd7dde08-f4f2-450d-b0f3-bc067d00f6d5"
      },
      "source": [
        "print(\"La media de la distribución de probabilidad de cada máquina es:\", medias)\n",
        "print(\"El valor esperado del premio por máquina es: \", Q)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La media de la distribución de probabilidad de cada máquina es: [-1.53818893 10.88372968  7.86945589 -2.6112537 ]\n",
            "El valor esperado del premio por máquina es:  [[-1.05727839  9.65014285  6.93915666 -2.53870777]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}